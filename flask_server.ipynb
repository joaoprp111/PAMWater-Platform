{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "based-attraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify, Response, request, after_this_request\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.tree import _tree\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "#from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "'''from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "import seaborn as sns \n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt'''\n",
    "from flask import send_file\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "encouraging-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'HELLO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "passing-camel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indicator_name</th>\n",
       "      <th>indicator_type</th>\n",
       "      <th>units</th>\n",
       "      <th>sub_type</th>\n",
       "      <th>input</th>\n",
       "      <th>value</th>\n",
       "      <th>date</th>\n",
       "      <th>city_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16103</th>\n",
       "      <td>azoto_total</td>\n",
       "      <td>Controlo Analitico</td>\n",
       "      <td>mg/l</td>\n",
       "      <td>Afluente Bruto</td>\n",
       "      <td>Entrada</td>\n",
       "      <td>68.000</td>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>Vila Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16104</th>\n",
       "      <td>azoto_total</td>\n",
       "      <td>Controlo Analitico</td>\n",
       "      <td>mg/l</td>\n",
       "      <td>Afluente Bruto</td>\n",
       "      <td>Entrada</td>\n",
       "      <td>76.000</td>\n",
       "      <td>2018-08-08</td>\n",
       "      <td>Vila Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16105</th>\n",
       "      <td>azoto_total</td>\n",
       "      <td>Controlo Analitico</td>\n",
       "      <td>mg/l</td>\n",
       "      <td>Afluente Bruto</td>\n",
       "      <td>Entrada</td>\n",
       "      <td>72.000</td>\n",
       "      <td>2018-08-15</td>\n",
       "      <td>Vila Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16106</th>\n",
       "      <td>azoto_total</td>\n",
       "      <td>Controlo Analitico</td>\n",
       "      <td>mg/l</td>\n",
       "      <td>Afluente Bruto</td>\n",
       "      <td>Entrada</td>\n",
       "      <td>63.000</td>\n",
       "      <td>2018-08-22</td>\n",
       "      <td>Vila Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16107</th>\n",
       "      <td>azoto_total</td>\n",
       "      <td>Controlo Analitico</td>\n",
       "      <td>mg/l</td>\n",
       "      <td>Afluente Bruto</td>\n",
       "      <td>Entrada</td>\n",
       "      <td>77.000</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>Vila Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22032</th>\n",
       "      <td>ortofosfatos</td>\n",
       "      <td>Controlo Analitico</td>\n",
       "      <td>mg/l</td>\n",
       "      <td>Efluente Tratado</td>\n",
       "      <td>Saida</td>\n",
       "      <td>4.470</td>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>Vila Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22033</th>\n",
       "      <td>ortofosfatos</td>\n",
       "      <td>Controlo Analitico</td>\n",
       "      <td>mg/l</td>\n",
       "      <td>Efluente Tratado</td>\n",
       "      <td>Saida</td>\n",
       "      <td>1.320</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>Vila Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22034</th>\n",
       "      <td>ortofosfatos</td>\n",
       "      <td>Controlo Analitico</td>\n",
       "      <td>mg/l</td>\n",
       "      <td>Efluente Tratado</td>\n",
       "      <td>Saida</td>\n",
       "      <td>2.270</td>\n",
       "      <td>2020-05-11</td>\n",
       "      <td>Vila Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22035</th>\n",
       "      <td>ortofosfatos</td>\n",
       "      <td>Controlo Analitico</td>\n",
       "      <td>mg/l</td>\n",
       "      <td>Efluente Tratado</td>\n",
       "      <td>Saida</td>\n",
       "      <td>0.958</td>\n",
       "      <td>2020-05-18</td>\n",
       "      <td>Vila Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22036</th>\n",
       "      <td>ortofosfatos</td>\n",
       "      <td>Controlo Analitico</td>\n",
       "      <td>mg/l</td>\n",
       "      <td>Efluente Tratado</td>\n",
       "      <td>Saida</td>\n",
       "      <td>3.140</td>\n",
       "      <td>2020-05-25</td>\n",
       "      <td>Vila Real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2065 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      indicator_name      indicator_type units          sub_type    input  \\\n",
       "16103    azoto_total  Controlo Analitico  mg/l    Afluente Bruto  Entrada   \n",
       "16104    azoto_total  Controlo Analitico  mg/l    Afluente Bruto  Entrada   \n",
       "16105    azoto_total  Controlo Analitico  mg/l    Afluente Bruto  Entrada   \n",
       "16106    azoto_total  Controlo Analitico  mg/l    Afluente Bruto  Entrada   \n",
       "16107    azoto_total  Controlo Analitico  mg/l    Afluente Bruto  Entrada   \n",
       "...              ...                 ...   ...               ...      ...   \n",
       "22032   ortofosfatos  Controlo Analitico  mg/l  Efluente Tratado    Saida   \n",
       "22033   ortofosfatos  Controlo Analitico  mg/l  Efluente Tratado    Saida   \n",
       "22034   ortofosfatos  Controlo Analitico  mg/l  Efluente Tratado    Saida   \n",
       "22035   ortofosfatos  Controlo Analitico  mg/l  Efluente Tratado    Saida   \n",
       "22036   ortofosfatos  Controlo Analitico  mg/l  Efluente Tratado    Saida   \n",
       "\n",
       "        value       date  city_name  \n",
       "16103  68.000 2018-08-01  Vila Real  \n",
       "16104  76.000 2018-08-08  Vila Real  \n",
       "16105  72.000 2018-08-15  Vila Real  \n",
       "16106  63.000 2018-08-22  Vila Real  \n",
       "16107  77.000 2018-08-29  Vila Real  \n",
       "...       ...        ...        ...  \n",
       "22032   4.470 2020-04-27  Vila Real  \n",
       "22033   1.320 2020-05-04  Vila Real  \n",
       "22034   2.270 2020-05-11  Vila Real  \n",
       "22035   0.958 2020-05-18  Vila Real  \n",
       "22036   3.140 2020-05-25  Vila Real  \n",
       "\n",
       "[2065 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''mydb = mysql.connector.connect(\n",
    "  host=\"freedb.tech\",\n",
    "  port=3306,\n",
    "  user=\"freedbtech_etardata\",\n",
    "  passwd=\"etardata2021\",\n",
    "  database=\"freedbtech_etardatadb\"\n",
    ")'''\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  port=8881,\n",
    "  user=\"root\",\n",
    "  passwd=\"root\",\n",
    "  database=\"etar_data_final\"\n",
    ")\n",
    "\n",
    "cursor = mydb.cursor()\n",
    "\n",
    "query_indicator_electricidade = \"SELECT indicator_table.indicator_name, indicator_table.indicator_type, indicator_table.units, indicator_value_table.sub_type, indicator_value_table.input, indicator_value_table.value, indicator_value_table.date, indicator_value_table.city_name FROM indicator_table INNER JOIN indicator_value_table ON indicator_table.id = indicator_value_table.indicator\"\n",
    "cursor.execute(query_indicator_electricidade)\n",
    "result_indicator_electricidade = cursor.fetchall()\n",
    "\n",
    "data = pd.DataFrame((result_indicator_electricidade),columns=['indicator_name','indicator_type','units','sub_type','input','value','date','city_name'])\n",
    "data = data[data.indicator_type == 'Controlo Analitico']\n",
    "data = data.loc[((data.sub_type == 'Afluente Bruto') | (data.sub_type == 'Efluente Tratado'))]\n",
    "data.date = pd.to_datetime(data.date).dt.date\n",
    "data.date = pd.to_datetime(data.date)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf06afcf-9744-49d1-8e2a-3de1b4963e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, timesteps, multisteps, dropnan=False, fill_value=0):\n",
    "    data = pd.DataFrame(data)\n",
    "    new = pd.DataFrame()\n",
    "    for i in range(timesteps, 0, -1):\n",
    "        if fill_value:\n",
    "            new = pd.concat([new, data.shift(i, fill_value=fill_value)], axis=1)\n",
    "        else:\n",
    "            new = pd.concat([new, data.shift(i)], axis=1)\n",
    "\n",
    "    for j in range(0, multisteps):\n",
    "        if fill_value:\n",
    "            new = pd.concat([new, data.iloc[:,0].shift(-j, fill_value=fill_value)],axis=1)\n",
    "        else:\n",
    "            new = pd.concat([new, data.iloc[:,0].shift(-j)],axis=1)\n",
    "    if dropnan:\n",
    "        new.dropna(inplace=True)\n",
    "    return new.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "variable-period",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_dates = pd.to_datetime(dados_forecast['date'])\\n\\ncols = list(dados_forecast)[1:7]\\n\\ndf_for_training = dados_forecast[cols].astype(float) \\nscaler = scaler.fit(df_for_training)\\ndf_for_training_scaled = scaler.transform(df_for_training)\\nX_train = []\\ny_train = []\\n\\nn_future = 1\\nn_past = 3     \\n\\nfor i in range(n_past, len(df_for_training_scaled) - n_future +1):\\n    X_train.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\\n    y_train.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])\\n    \\nX_train, y_train = np.array(X_train), np.array(y_train)\\ndisplay(X_train, X_train.shape, y_train, y_train.shape)\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = [0, 91, 183, 275, 366]\n",
    "labels=['Inverno', 'Primavera', 'Verão', 'Outono']\n",
    "\n",
    "for i in data.indicator_name.unique():\n",
    "    dados_x = data[data.indicator_name == i]\n",
    "    for p, o in dados_x.groupby(\"sub_type\"):\n",
    "        o.date = pd.to_datetime(o.date).dt.date\n",
    "        o.date = pd.to_datetime(o.date)\n",
    "        o = o.groupby('sub_type').resample('7D', on='date').mean().reset_index().sort_values(by='date')\n",
    "        datas_falta = pd.date_range(start = o.date.min(), end = o.date.max(), freq=\"7D\").difference(o.date)\n",
    "        datas_em_falta = dados_x.reindex(datas_falta, fill_value = 0).reset_index()\n",
    "        datas_em_falta.drop(columns=['date'], inplace = True)\n",
    "        datas_em_falta.rename(columns={'index': 'date'}, inplace = True)\n",
    "        for o, p in datas_em_falta.iterrows():\n",
    "            data = data.append({ 'indicator_name': i, 'sub_type': p, 'date':p.date, 'value':p.value}, ignore_index=True)\n",
    "datasets = []\n",
    "for x in data.indicator_name.unique():\n",
    "    d = data[data.indicator_name == x]\n",
    "    for i in d.sub_type.unique():\n",
    "        dados_x = d[d.sub_type == i].copy()\n",
    "        dados_x.date = pd.to_datetime(dados_x.date).dt.date\n",
    "        dados_x.date = pd.to_datetime(dados_x.date)\n",
    "\n",
    "        dados_x.date = dados_x.date.dt.to_period('W').apply(lambda r: r.start_time)\n",
    "        dados_x = dados_x.groupby([dados_x['date'],dados_x['indicator_name'],dados_x['sub_type'], dados_x['units']]).aggregate('mean').reset_index()\n",
    "        #dados_x = dados_x.groupby(['indicator_name','sub_type','units']).resample('', on='date').mean().reset_index().sort_values(by='date')\n",
    "\n",
    "        dados_x = dados_x.loc[dados_x.notnull().all(axis=1).cummax()]\n",
    "        nan = dados_x[dados_x.isnull().any(1)]\n",
    "        if len(nan) > 0:\n",
    "            idx = dados_x.loc[dados_x.date == nan.date.iloc[0]]\n",
    "            num_timesteps = 3\n",
    "            while (len(dados_x.loc[:idx.index[0]]) - 1) < num_timesteps:\n",
    "                dados_x.at[idx.index[0],'value'] = dados_x.loc[:idx.index[0]].mean()\n",
    "                nan = dados_x[dados_x.isnull().any(1)]\n",
    "                if len(nan) == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    idx = dados_x.loc[dados_x.date == nan.date.iloc[0]]\n",
    "            while int(dados_x.value.isnull().sum()) > 0:\n",
    "                dados_x.value = dados_x.value.fillna(dados_x.value.rolling(num_timesteps).mean().shift())\n",
    "        datasets.append(dados_x)\n",
    "dados_final = pd.concat(datasets)\n",
    "\n",
    "dados_con_analitico = dados_final.copy()\n",
    "for i,p in dados_con_analitico.iterrows():\n",
    "    dados_con_analitico.loc[i, [p.indicator_name + \" em \" + p.sub_type]] = np.nan\n",
    "    dados_con_analitico.loc[i, [p.indicator_name + \" em \" + p.sub_type]] = p.value\n",
    "dados_con_analitico = dados_con_analitico.drop(columns=['value'])\n",
    "dados_con_analitico = dados_con_analitico.groupby('date').aggregate('mean').reset_index()\n",
    "for x in dados_con_analitico.columns:\n",
    "    dados_x = dados_con_analitico[x]\n",
    "    if dados_x.isnull().sum() > 0:\n",
    "        while int(dados_x.isnull().sum()) > 0:\n",
    "                dados_x = dados_x.fillna(dados_x.rolling(3).mean().shift())\n",
    "        dados_con_analitico[x] = dados_x\n",
    "doy = dados_con_analitico.date.dt.dayofyear\n",
    "dados_con_analitico['estacao_ano'] = pd.cut(doy + 11 - 366*(doy > 355), bins=bins, labels=labels)\n",
    "dados_con_analitico['estacao_ano_numeric'] = dados_con_analitico.estacao_ano.cat.codes\n",
    "dados_forecast = dados_con_analitico[['date','azoto_total em Efluente Tratado','cqo em Efluente Tratado','amonia em Efluente Tratado']]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "dados_super = series_to_supervised(dados_forecast.loc[:,dados_forecast.columns != 'date'], 6, 3, dropnan = True)\n",
    "\n",
    "'''train_dates = pd.to_datetime(dados_forecast['date'])\n",
    "\n",
    "cols = list(dados_forecast)[1:7]\n",
    "\n",
    "df_for_training = dados_forecast[cols].astype(float) \n",
    "scaler = scaler.fit(df_for_training)\n",
    "df_for_training_scaled = scaler.transform(df_for_training)\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "n_future = 1\n",
    "n_past = 3     \n",
    "\n",
    "for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n",
    "    X_train.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "    y_train.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])\n",
    "    \n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "display(X_train, X_train.shape, y_train, y_train.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reverse-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/dados')\n",
    "def dados():\n",
    "    @after_this_request\n",
    "    def add_header(response):\n",
    "        response.headers['Access-Control-Allow-Origin'] = '*'\n",
    "        return response\n",
    "    #result = data.to_json(orient=\"records\")\n",
    "    return Response(data.to_json(orient=\"records\"), mimetype='application/json')#jsonify(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "valuable-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/last_date')\n",
    "def last_date():\n",
    "    @after_this_request\n",
    "    def add_header(response):\n",
    "        response.headers['Access-Control-Allow-Origin'] = '*'\n",
    "        return response\n",
    "    last_date = dados_forecast.date.iloc[-1]\n",
    "    res = last_date.strftime(\"%d/%m/%Y\")\n",
    "    return jsonify(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expected-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/prediction', methods=['GET','POST'])\n",
    "def prediction():\n",
    "    @after_this_request\n",
    "    def add_header(response):\n",
    "        response.headers['Access-Control-Allow-Origin'] = '*'\n",
    "        return response\n",
    "    if request.method == 'POST':\n",
    "        #result_post = request.form\n",
    "        pred = [[request.form['azoto_total_em_Afluente_Bruto'], request.form['cqo_em_Efluente_Tratado'], request.form['sst_em_Afluente_Bruto'], request.form['amonia_em_Efluente_Tratado'],\trequest.form['ortofosfatos_em_Efluente_Tratado']]]\n",
    "    loaded_model = joblib.load('dt_model.sav')\n",
    "    prediction = loaded_model.predict(pred)\n",
    "    result = {'azoto_total_em_Afluente_Bruto':request.form['azoto_total_em_Afluente_Bruto'], 'cqo_em_Efluente_Tratado': request.form['cqo_em_Efluente_Tratado'], 'sst_em_Afluente_Bruto':request.form['sst_em_Afluente_Bruto'], 'amonia_em_Efluente_Tratado': request.form['amonia_em_Efluente_Tratado'], 'ortofosfatos_em_Efluente_Tratado': request.form['ortofosfatos_em_Efluente_Tratado'], 'previsao': str(prediction[0])}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mysterious-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        \n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "            p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [f\"({name} > {np.round(threshold, 3)})\"]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = \"Se \"\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            if rule != \"Se \":\n",
    "                rule += \" e \"\n",
    "            rule += str(p)\n",
    "        rule += \" então \"\n",
    "        if class_names is None:\n",
    "            rule += \"o valor é: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            rule += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "        #rule += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rules += [rule]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "guilty-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/rules')\n",
    "def rules():\n",
    "    rules_array=[]\n",
    "    @after_this_request\n",
    "    def add_header(response):\n",
    "        response.headers['Access-Control-Allow-Origin'] = '*'\n",
    "        return response\n",
    "    #result_post = request.form\n",
    "    loaded_model = joblib.load('dt_model.sav')\n",
    "    dataframe_for_columns_name = pd.DataFrame({'Azoto total em Afluente Bruto': 111, 'CQO em Efluente Tratado': 111, 'SST em Afluente Bruto': 111, 'Amonia em Efluente Tratado': 111, 'Ostofosfatos em Efluente Tratado': 111}, index=[0])\n",
    "    rules = get_rules(loaded_model, dataframe_for_columns_name.columns, None)\n",
    "    for r in rules:\n",
    "        rules_array.append(r)\n",
    "    return jsonify(rules_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "immune-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/prediction_next_days')\n",
    "def predict_future():\n",
    "    @after_this_request\n",
    "    def add_header(response):\n",
    "        response.headers['Access-Control-Allow-Origin'] = '*'\n",
    "        return response \n",
    "    #if request.method == 'POST':\n",
    "        #pred_weeks = [[request.form['weeks_pred']]]\n",
    "\n",
    "    model = load_model('lstm_AT.h5')\n",
    "    df_dates = dados_forecast.date\n",
    "    dados_f = dados_super[:, :-3]\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    scaler = scaler.fit(dados_f)\n",
    "    df_scaled = scaler.transform(dados_f)\n",
    "\n",
    "    df_scaled = df_scaled.reshape(-1,6,3)\n",
    "    forecast_period_dates = pd.date_range(list(df_dates)[-1], periods=3 + 1, freq='7D').tolist()\n",
    "    forecast = model.predict(df_scaled[-1:])\n",
    "    final_forecast = list()\n",
    "    for i in forecast[0]:\n",
    "        forecast_copies = np.repeat([[i]], dados_f.shape[1], axis=-1)\n",
    "        y_pred_future = scaler.inverse_transform(forecast_copies)[:,0]\n",
    "        final_forecast.append(y_pred_future)\n",
    "    forecast_dates = []\n",
    "    for time_i in forecast_period_dates:\n",
    "        forecast_dates.append(time_i.date())\n",
    "    forecast_dates.pop(0)\n",
    "\n",
    "    df_forecast = pd.DataFrame({'date':np.ravel(forecast_dates), 'azoto_total_em_Efluente_Tratado_pred': np.ravel(final_forecast)})\n",
    "    df_forecast['date']=pd.to_datetime(df_forecast['date'])\n",
    "\n",
    "    original = dados_forecast[['date', 'azoto_total em Efluente Tratado']]\n",
    "    original['date']=pd.to_datetime(original['date'])\n",
    "    #original = original.loc[original['date'] >= '2020-4-1']\n",
    "    original = original.iloc[-8:]\n",
    "\n",
    "    global prev_data, g_original\n",
    "    prev_data = df_forecast\n",
    "\n",
    "\n",
    "    '''n_future_weeks = int(pred_weeks[0][0])\n",
    "    forecast_period_dates = pd.date_range(list(train_dates)[-1], periods=n_future_weeks + 1, freq='7D').tolist()\n",
    "    forecast = model.predict(X_train[-n_future_weeks:])\n",
    "    forecast_copies = np.repeat(forecast, df_for_training.shape[1], axis=-1)\n",
    "    y_pred_future = scaler.inverse_transform(forecast_copies)[:,0]\n",
    "\n",
    "    forecast_dates = []\n",
    "    for time_i in forecast_period_dates:\n",
    "        forecast_dates.append(time_i.date())\n",
    "    forecast_dates.pop(0)\n",
    "    df_forecast = pd.DataFrame({'date':np.ravel(forecast_dates), 'azoto_total_em_Efluente_Tratado_pred': y_pred_future})\n",
    "    df_forecast['date']=pd.to_datetime(df_forecast['date'])\n",
    "    df_forecast.azoto_total_em_Efluente_Tratado_pred = df_forecast.azoto_total_em_Efluente_Tratado_pred.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    original = dados_forecast[['date', 'azoto_total em Efluente Tratado']]\n",
    "    original['date']=pd.to_datetime(original['date'])\n",
    "    #original = original.loc[original['date'] >= '2020-4-1']\n",
    "    original = original.iloc[-8:]\n",
    "\n",
    "    global prev_data, g_original\n",
    "    prev_data = df_forecast'''\n",
    "    g_original = original\n",
    "    #filename = 'graph.png'\n",
    "    #return send_file(filename, mimetype='image/png')\n",
    "    res = pd.DataFrame({'date_ori': g_original.date.astype(str), 'values_ori':g_original['azoto_total em Efluente Tratado'], 'pred_dates':prev_data.date.astype(str), 'pred_values': prev_data['azoto_total_em_Efluente_Tratado_pred']})\n",
    "    return Response(res.round(3).to_json(orient=\"records\"), mimetype='application/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abstract-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/prediction_next_days_values')\n",
    "def prediction_next_days_values():\n",
    "    @after_this_request\n",
    "    def add_header(response):\n",
    "        response.headers['Access-Control-Allow-Origin'] = '*'\n",
    "        return response\n",
    "    prediction_values = {'pred_date':prev_data['date'], 'pred_val':prev_data['azoto_total_em_Efluente_Tratado_pred']}  \n",
    "    prediction_values = pd.DataFrame(prediction_values)\n",
    "    #prediction_values.sort_values(by=['pred_date'], inplace=True, ascending=False)\n",
    "    prediction_values[\"pred_date\"] = prediction_values[\"pred_date\"].astype(str)\n",
    "    return Response(prediction_values.to_json(orient=\"records\"), mimetype='application/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f90d2ff-e14a-42f3-ba2a-396d50bec13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/insert_data',  methods=['POST'])\n",
    "def insert_data():\n",
    "    @after_this_request\n",
    "    def add_header(response):\n",
    "        response.headers['Access-Control-Allow-Origin'] = '*'\n",
    "        return response \n",
    "    if request.method == 'POST':\n",
    "        csv = request.form.to_dict(flat=False)\n",
    "        final_csv = pd.DataFrame(csv)\n",
    "        final_csv = final_csv['rows[]'].str.split(',', expand=True)\n",
    "        final_csv.rename(columns=final_csv.iloc[0], inplace=True)\n",
    "        final_csv = final_csv.iloc[1:, :]\n",
    "        final_csv.dropna(inplace=True)\n",
    "        display(final_csv)\n",
    "        \n",
    "    \n",
    "    return (str('ola dani'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mighty-beads",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [18/Nov/2021 14:54:34] \"\u001b[37mGET /last_date HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tese/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tese/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "127.0.0.1 - - [18/Nov/2021 14:54:42] \"\u001b[37mGET /prediction_next_days HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [18/Nov/2021 14:54:42] \"\u001b[37mGET /prediction_next_days_values HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
